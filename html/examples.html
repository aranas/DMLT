
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>examples</title>
      <meta name="generator" content="MATLAB 7.7">
      <meta name="date" content="2011-12-17">
      <meta name="m-file" content="examples"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head>
   <body>
      <div class="content">
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="#1">Examples</a></li>
               <li><a href="#2">Elastic net regression</a></li>
               <li><a href="#9">Bayesian logistic regression with a multivariate Laplace prior</a></li>
               <li><a href="#10">Sparse orthonormalized partial least squares</a></li>
               <li><a href="#11">Dealing with neural data</a></li>
            </ul>
         </div>
         <h2>Examples<a name="1"></a></h2>
         <p>In the following examples we will make use of a neuroimaging dataset acquired at the Donders Institute. This data and a description
            can be downloaded from https://sites.google.com/site/cogneurodynamics/data
         </p><pre class="codeinput">load <span class="string">69digits</span>
</pre><h2>Elastic net regression<a name="2"></a></h2>
         <p>Elastic net regression can be used to get sparse solutions where regression coefficient vectors consist of many zeros. Elastic
            net regression allows for linear and logistic regression depending on the 'family' parameter ('gaussian' or 'binomial'). Here,
            we deal with logistic regression only and focus on classification. This toolbox implements two versions of elastic net: dml.enet
            and dml.glmnet. The former one is more general and implemented in Matlab. The latter one is less general and calls external
            Fortran code, making it much faster. In the following, we show how to use these two implementations on our example data.
         </p>
         <p>dml.enet minimizes the sum squared error on the data plus a regularization term of the form:</p>
         <p><img src="examples_eq62684.png" alt="$$L_1 ||\beta||_1 + \frac{1}{2} \beta' L_2 \beta$$"></p>
         <p>where the betas are the regression coefficients, L1 is a scalar and L2 is a scalar or a matrix.  The L1 and L2 parameters
            provide a different form of regularization, with L1 giving sparse solutions and L2 giving smooth solutions. For the moment,
            we fix L2 to a small constant (default is 1e-6) and want to find the solution for L1=0.1.
         </p><pre class="codeinput">m = dml.enet(<span class="string">'family'</span>,<span class="string">'binomial'</span>,<span class="string">'L1'</span>,0.1);
m = m.train(X,Y);
</pre><p>The sparse solution is returned in the model.weights field:</p><pre class="codeinput">bar(m.model.weights);
</pre><img vspace="5" hspace="5" src="examples_01.png" alt=""> <p>Note however that directly finding the solution for a small L1 value can be numerically unstable. It's better to approximate
            this solution using small decreases in the L1 value. Furthermore, typically, the value of L1 is unknown and one wants to find
            such a value. This is where the gridsearch comes into play. We first generate a suitable regularization path (values of L1)
            to follow using a helper function
         </p><pre class="codeinput">v = dml.enet.lambdapath(X,Y,<span class="string">'binomial'</span>,50,1e-2);
</pre><p>and then we use it in a gridsearch</p><pre class="codeinput">m = dml.gridsearch(<span class="string">'cv'</span>,dml.crossvalidator(<span class="string">'type'</span>,<span class="string">'split'</span>,<span class="string">'stat'</span>,<span class="string">'accuracy'</span>,<span class="string">'mva'</span>,dml.enet(<span class="string">'family'</span>,<span class="string">'binomial'</span>,<span class="string">'restart'</span>,false)),<span class="string">'vars'</span>,<span class="string">'L1'</span>,<span class="string">'vals'</span>,v);
tic; m = m.train(X,Y); toc
</pre><pre class="codeoutput">Elapsed time is 16.842036 seconds.
</pre><p>In order to see how performance changes as a function of L1, we can use the following code:</p><pre class="codeinput">plot(m.configs,m.outcome); xlabel(<span class="string">'L1'</span>); ylabel(<span class="string">'accuracy'</span>)
</pre><img vspace="5" hspace="5" src="examples_02.png" alt=""> <p>The associated configuration of parameter values is used to retrain the model on all data. This model can be accessed using
            the m.model field. Note that this model can differ from the model(s) obtained using crossvalidation since all data is used:
         </p><pre class="codeinput">subplot(1,2,1); bar(m.models{m.optimum}.weights);
subplot(1,2,2); bar(m.model.weights);
</pre><img vspace="5" hspace="5" src="examples_03.png" alt=""> <p>The same results can be achieved using dml.glmnet. However, here the regularization term is a bit different:</p>
         <p><img src="examples_eq33877.png" alt="$$\lambda( (1-\alpha) \frac{1}{2}||\beta||^2_2 + \alpha ||\beta||_1 )$$"></p>
         <p>Furthermore, the cross-validation and determination of the regularization path is done automatically inside the model. We
            can call this code as follows:
         </p><pre class="codeinput">m = dml.glmnet;
tic; m = m.train(X,Y); toc;
close; bar(m.model.weights);
</pre><pre class="codeoutput">Elapsed time is 1.397489 seconds.
</pre><img vspace="5" hspace="5" src="examples_04.png" alt=""> <h2>Bayesian logistic regression with a multivariate Laplace prior<a name="9"></a></h2>
         <h2>Sparse orthonormalized partial least squares<a name="10"></a></h2>
         <h2>Dealing with neural data<a name="11"></a></h2>
         <p>MLN also offers support for handling and visualizing neural data. For instance, suppose we trained the following model:</p><pre class="codeinput">m = dml.naive;
m = m.train(X,Y);
w = m.model.divergence;
</pre><p>Then, we can plot the contributions to the decoding back to brain space as follows:</p><pre class="codeinput">M=mask; M(M) = w;
subplot(1,2,1); imagesc(mean(abs(M),3)); axis <span class="string">off</span>; axis <span class="string">square</span>;
subplot(1,2,2); imagesc(mean(structural,3)); axis <span class="string">off</span>; axis <span class="string">square</span>
</pre><img vspace="5" hspace="5" src="examples_05.png" alt=""> <p>It may be more convenient however to write these files to NIFTI format such that the data can be handled by standard fMRI
            packages:
         </p>
         <p class="footer"><br>
            Published with MATLAB&reg; 7.7<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Examples
% In the following examples we will make use of a neuroimaging dataset
% acquired at the Donders Institute. This data and a description can be
% downloaded from https://sites.google.com/site/cogneurodynamics/data
load 69digits

%% Elastic net regression
% Elastic net regression can be used to get sparse solutions where
% regression coefficient vectors consist of many zeros. Elastic net
% regression allows for linear and logistic regression depending on the
% 'family' parameter ('gaussian' or 'binomial'). Here, we deal with
% logistic regression only and focus on classification. This toolbox
% implements two versions of elastic net: dml.enet and dml.glmnet. The
% former one is more general and implemented in Matlab. The latter one is
% less general and calls external Fortran code, making it much faster. In
% the following, we show how to use these two implementations on our
% example data.
%
% dml.enet minimizes the sum squared error on the data plus a
% regularization term of the form:
%
% $$L_1 ||\beta||_1 + \frac{1}{2} \beta' L_2 \beta$$
%
% where the betas are the regression coefficients, L1 is a scalar and L2 is
% a scalar or a matrix.  The L1 and L2 parameters provide a different form
% of regularization, with L1 giving sparse solutions and L2 giving smooth
% solutions. For the moment, we fix L2 to a small constant (default is
% 1e-6) and want to find the solution for L1=0.1.
m = dml.enet('family','binomial','L1',0.1);
m = m.train(X,Y);
%% 
% The sparse solution is returned in the model.weights field:
bar(m.model.weights); 
%%
% Note however that directly finding the solution for a small L1 value can
% be numerically unstable. It's better to approximate this solution using
% small decreases in the L1 value. Furthermore, typically, the value of L1
% is unknown and one wants to find such a value. This is where the
% gridsearch comes into play. We first generate a suitable regularization
% path (values of L1) to follow using a helper function
v = dml.enet.lambdapath(X,Y,'binomial',50,1e-2);
%%
% and then we use it in a gridsearch
m = dml.gridsearch('cv',dml.crossvalidator('type','split','stat','accuracy','mva',dml.enet('family','binomial','restart',false)),'vars','L1','vals',v);
tic; m = m.train(X,Y); toc
%%
% In order to see how performance changes as a function of L1, we can use
% the following code:
plot(m.configs,m.outcome); xlabel('L1'); ylabel('accuracy')
%%
% The associated configuration of parameter values is used to
% retrain the model on all data. This model can be accessed using the
% m.model field. Note that this model can differ from the model(s) obtained
% using crossvalidation since all data is used:
subplot(1,2,1); bar(m.models{m.optimum}.weights);
subplot(1,2,2); bar(m.model.weights);

%%
% The same results can be achieved using dml.glmnet. However, here the regularization
% term is a bit different:
%
% $$\lambda( (1-\alpha) \frac{1}{2}||\beta||^2_2 + \alpha ||\beta||_1 )$$ 
%
% Furthermore, the cross-validation and determination of the regularization 
% path is done automatically inside the model. We can call this code as follows:
m = dml.glmnet;
tic; m = m.train(X,Y); toc;
close; bar(m.model.weights);

%% Bayesian logistic regression with a multivariate Laplace prior

%% Sparse orthonormalized partial least squares

%% Dealing with neural data
%
% MLN also offers support for handling and visualizing neural data. For
% instance, suppose we trained the following model:
m = dml.naive;
m = m.train(X,Y);
w = m.model.divergence;
%%
% Then, we can plot the contributions to the decoding back to brain space as follows:
M=mask; M(M) = w;
subplot(1,2,1); imagesc(mean(abs(M),3)); axis off; axis square; 
subplot(1,2,2); imagesc(mean(structural,3)); axis off; axis square
%%
% It may be more convenient however to write these files to NIFTI format
% such that the data can be handled by standard fMRI packages:


##### SOURCE END #####
-->
   </body>
</html>